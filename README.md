# ğŸ” Privacy-Preserving Machine Learning for Diabetes Disease Classification

This project explores the application of **Differential Privacy (DP)** to machine learning models for **diabetes classification** using the **Pima Indians Diabetes Dataset**. It evaluates the **privacy-utility trade-off** across various privacy budgets (Îµ) using **Logistic Regression** with:

- âœ… Objective Perturbation  
- âœ… Output Perturbation  
- âœ… Non-Private Baseline  

---

## ğŸ“Š Dataset

- **Name**: Pima Indians Diabetes Dataset  
- **Instances**: 768 female patients (21+ years old)  
- **Features**: 8 diagnostic attributes  
- **Target**: `Outcome` (0 = Non-diabetic, 1 = Diabetic)

ğŸ”— [Download Dataset](https://drive.google.com/file/d/14aqc4rK8hcFSES-SlRztGBlGca-QuIDd/view?usp=sharing)

---

## ğŸ” Key Techniques

### 1. Objective Perturbation
- Adds **Gaussian noise** to the loss function.
- Ensures **(Îµ, Î´)-Differential Privacy**.
- Optimized using **L-BFGS-B**.

### 2. Output Perturbation
- Adds **Laplace noise** to trained model weights.
- Ensures **Îµ-Differential Privacy**.
- Simpler, but less utility at strict privacy levels.

### 3. Noise Mechanisms Used
- **Laplace Mechanism**  
- **Gaussian Mechanism**  
- **Exponential Mechanism**  
- **Randomized Response**  
- **Shuffling Mechanism**

---

## âš™ï¸ Implementation Highlights

- ğŸ”§ Feature scaling using `StandardScaler`
- ğŸ”„ Train-test split (80/20) with stratified sampling
- ğŸ“ˆ Evaluation Metrics:
  - Accuracy
  - Precision
  - Recall
  - F1 Score
  - AUC-ROC
- ğŸ“‰ Privacy-Utility Tradeoff Analysis
- ğŸ“Š Visualization of noise impact on data distribution

---

## ğŸ“ˆ Privacy-Utility Insights

- **Lower Îµ â†’ Higher Privacy â†’ Lower Accuracy**
- **Higher Îµ â†’ Lower Privacy â†’ Higher Accuracy**

### ğŸ” Results Summary:
- Objective Perturbation outperforms Output Perturbation, especially at low Îµ.
- Epsilon in **1.0 â€“ 5.0** provides a practical balance of privacy and utility.
- Visualization shows progressive data distortion as Îµ decreases.

---

## ğŸ§  Conclusion

This implementation demonstrates how **Differential Privacy** can be effectively applied to **healthcare datasets**, preserving **patient confidentiality** while maintaining strong **predictive performance**. It offers a practical framework for privacy-aware machine learning in sensitive domains.

---

## ğŸ“ Code & Data

- ğŸ“‚ [Code Repository](https://drive.google.com/file/d/1uV9FGOaUDWaq31RHoEL_Vu2NB6hf5Wm_/view?usp=sharing)
- ğŸ“„ [Dataset Link](https://drive.google.com/file/d/14aqc4rK8hcFSES-SlRztGBlGca-QuIDd/view?usp=sharing)

---

