# pavitha16-Privacy-Preserving-Machine-Learning-for-Diabetes-Disease-
##ğŸ” Privacy-Preserving Machine Learning for Diabetes Classification##
This project demonstrates the application of Differential Privacy (DP) techniques to machine learning models for diabetes classification using the Pima Indians Diabetes Dataset. It compares model performance and privacy guarantees across different privacy budgets (Îµ) using logistic regression with:

âœ… Objective Perturbation

âœ… Output Perturbation

âœ… Non-Private Baseline

##ğŸ“Š Dataset##
Pima Indians Diabetes Dataset

768 female patients (21+ years)

8 diagnostic features

Binary target: Outcome (0: Non-diabetic, 1: Diabetic)

ğŸ“¥ Download Dataset

ğŸ” Key Techniques
1. Objective Perturbation
Adds Gaussian noise to the loss function. Ensures (Îµ, Î´)-DP.

2. Output Perturbation
Adds Laplace noise to model weights post-training. Ensures Îµ-DP.

3. Noise Mechanisms Used
Laplace

Gaussian

Exponential

Randomized Response

Shuffling

âš™ï¸ Implementation Highlights
Feature Scaling with StandardScaler

Logistic Regression with/without Differential Privacy

Evaluation: Accuracy, AUC-ROC, Precision, Recall, F1

Privacy-Utility Tradeoff Analysis

ğŸ“ˆ Lower Îµ = Higher privacy + Lower accuracy
ğŸ“ˆ Higher Îµ = Lower privacy + Better accuracy

ğŸ“Œ Results
Objective Perturbation maintains better utility than Output Perturbation.

Epsilon values in the 1.0 â€“ 5.0 range strike a practical balance between privacy and performance.

Visualization shows impact of noise at different privacy levels.

ğŸ§  Conclusion
This implementation provides a practical framework to apply Differential Privacy in healthcare, balancing data confidentiality with model effectiveness.
